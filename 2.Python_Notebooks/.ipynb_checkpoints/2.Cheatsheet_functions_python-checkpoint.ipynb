{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as ss\n",
    "from scipy.stats import probplot\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import os\n",
    "import sys\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to standardize column names in a dataframe\n",
    "def cols_name(df, max_cols_letters = 15):\n",
    "    \"\"\"Function for standardizing and shortening the columns names of a dataframe df. /n\n",
    "    Two arguments needed: dataframe and maximum letters for the columns names. /n\n",
    "    Replaces spaces with underscore and makes all lowercase, shortens the columns name to max_clos_letters /n\n",
    "    e.g Customer Lifetime value --> customer_lif\"\"\"\n",
    "    cols = df.columns\n",
    "    new_column_names =[]\n",
    "    for col in cols:\n",
    "        new_col = col.lower().replace (\" \", \"_\")\n",
    "        new_col = (new_col[:max_cols_letters]) if len(new_col) > max_cols_letters else new_col\n",
    "        new_column_names.append(new_col)\n",
    "    df.columns = new_column_names\n",
    "    return df\n",
    "\n",
    "#memory usage deep tolist var and nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display  missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to display missing values\n",
    "def display_missing(df):\n",
    "    \"\"\"shows the number of missing values of each column for a date_frame, if there is one\"\"\"\n",
    "    for col in df.columns.tolist():\n",
    "        if df[col].isnull().sum():\n",
    "            print('{} column missing values: {}/{}'.format(col, df[col].isnull().sum(), len(df)))\n",
    "    print ('Done checking for missing values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display datatypes of df, lists them in df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to display data type in a dataframe\n",
    "def cols_dtypes(df):\n",
    "    \"\"\"retrns data types of columns of a dataframe in form of a dataframe, the values beeing the feature/variable names\"\"\"\n",
    "    categoricals = df.select_dtypes(include='object').columns\n",
    "    numerics = df.select_dtypes(exclude='object').columns\n",
    "    booleans = df.select_dtypes(include='bool').columns\n",
    "    floats = df.select_dtypes(include='float').columns\n",
    "    integers = df.select_dtypes(include='int').columns\n",
    "    data_types = pd.DataFrame([categoricals,numerics, booleans, floats, integers])\n",
    "    data_types = data_types.T\n",
    "    data_types.columns=['catergoricals or mixed', 'nuermicals', 'booleans','floats', 'integers']\n",
    "    return data_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log transformation and boxcox for a feature, plots 3 distplot next to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "def feat_log_box(df, feature, bins = 100):\n",
    "    \"\"\"makes a log Transformation and a boxcox transformation of a feature of a dataframe. \\n\n",
    "    0 values are replaced with the mean in the transformation: \\n\n",
    "    plots 3 distplots next to each other for comparison \\n\n",
    "    tales 3 arguments (df, feature, bins)\"\"\"\n",
    "    df[feature+'_log'] = list(map(lambda x: np.log(x) if np.isfinite(x) and x!=0 else np.NAN, df[feature]))\n",
    "    df[feature+'_log'] = df[feature+'_log'].fillna(np.mean(df[feature+'_log']))\n",
    "    df[feature+'_boxcox'] = np.where(df[feature]<=0,0,df[feature])\n",
    "    mean = np.sum(df[feature+'_boxcox'])/len(df[df[feature+'_boxcox']>0])\n",
    "    df[feature+'_boxcox'] = df[feature+'_boxcox'].replace(0,mean)\n",
    "    xt, lmbda = stats.boxcox(df[feature+'_boxcox'])\n",
    "    df[feature+'_boxcox'] = xt\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (15,4))\n",
    "    sns.distplot(df[feature], bins, ax=ax1)\n",
    "    ax1.set_title(feature)\n",
    "    sns.distplot(df[feature+'_log'], bins, ax=ax2)\n",
    "    ax2.set_title(feature + '_log')\n",
    "    sns.distplot(xt, bins, ax=ax3)\n",
    "    ax3.set_title(feature + '_boxcox')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eta for correlation among categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculation of eta\n",
    "#https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9\n",
    "\n",
    "def correlation_ratio(categories, measurements):\n",
    "    fcat, _ = pd.factorize(categories)\n",
    "    cat_num = np.max(fcat)+1\n",
    "    y_avg_array = np.zeros(cat_num)\n",
    "    n_array = np.zeros(cat_num)\n",
    "    for i in range(0,cat_num):\n",
    "        cat_measures = measurements[np.argwhere(fcat == i).flatten()]\n",
    "        n_array[i] = len(cat_measures)\n",
    "        y_avg_array[i] = np.average(cat_measures)\n",
    "    y_total_avg = np.sum(np.multiply(y_avg_array,n_array))/np.sum(n_array)\n",
    "    numerator = np.sum(np.multiply(n_array,np.power(np.subtract(y_avg_array,y_total_avg),2)))\n",
    "    denominator = np.sum(np.power(np.subtract(measurements,y_total_avg),2))\n",
    "    if numerator == 0:\n",
    "        eta = 0.0\n",
    "    else:\n",
    "        eta = numerator/denominator\n",
    "    return eta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate cramers_v, phi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramers_v(x, y):\n",
    "    confusion_matrix = pd.crosstab(x,y)\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2/n\n",
    "    r,k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))\n",
    "    rcorr = r-((r-1)**2)/(n-1)\n",
    "    kcorr = k-((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model validation (needs dic with columnnames as keys and predict as values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "def regression_results(dic_of_models, X_test, y_true):\n",
    "    '''returns a dataframe with 'Model', 'explained_variance', 'r2', 'adjusted_r2', 'MAE', 'MSE', 'RMSE', 'MSLE',\n",
    "    need the dictionary of key='modelname', value=prediction, X_test, y_test as attributes'''\n",
    "    model_val = pd.DataFrame(columns =['Model', 'explained_variance', 'r2', 'adjusted_r2', 'MAE', 'MSE', 'RMSE', 'MSLE'])\n",
    "    # Regression metrics\n",
    "    for key, y_pred in dic_of_models.items():\n",
    "        explained_variance=metrics.explained_variance_score(y_true, y_pred)\n",
    "        mean_absolute_error=metrics.mean_absolute_error(y_true, y_pred) \n",
    "        mse=metrics.mean_squared_error(y_true, y_pred) \n",
    "        mean_squared_log_error=metrics.mean_squared_log_error(y_true, y_pred)\n",
    "        median_absolute_error=metrics.median_absolute_error(y_true, y_pred)\n",
    "        r2=metrics.r2_score(y_true, y_pred)\n",
    "        n = len(X_test)\n",
    "        p = X_test.shape[1]\n",
    "        adj_r2 = 1-((1-r2)*(n-1)/(n-p-1))\n",
    "    \n",
    "        val_list = [key, \n",
    "                explained_variance,\n",
    "                round(r2,4),\n",
    "                round(adj_r2,4),\n",
    "                round(mean_absolute_error,4),\n",
    "                round(mse,4),\n",
    "                round(np.sqrt(mse),4),\n",
    "                round(mean_squared_log_error,4)]\n",
    "        v_series = pd.Series(val_list, index = model_val.columns)\n",
    "        model_val = model_val.append(v_series, ignore_index=True)\n",
    "    return model_val\n",
    "\n",
    "#model_dic = {'lm1': predictions, 'lm2':predictions_2, 'knn_model_manhattan':knn_predict_manh}\n",
    "#validation=regression_results(model_dic, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### converter ('array, 'list', 'dataframe', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(data, to):\n",
    "    converted = None\n",
    "    if to == 'array':\n",
    "        if isinstance(data, np.ndarray):\n",
    "            converted = data\n",
    "        elif isinstance(data, pd.Series):\n",
    "            converted = data.values\n",
    "        elif isinstance(data, list):\n",
    "            converted = np.array(data)\n",
    "        elif isinstance(data, pd.DataFrame):\n",
    "            converted = data.as_matrix()\n",
    "    elif to == 'list':\n",
    "        if isinstance(data, list):\n",
    "            converted = data\n",
    "        elif isinstance(data, pd.Series):\n",
    "            converted = data.values.tolist()\n",
    "        elif isinstance(data, np.ndarray):\n",
    "            converted = data.tolist()\n",
    "    elif to == 'dataframe':\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            converted = data\n",
    "        elif isinstance(data, np.ndarray):\n",
    "            converted = pd.DataFrame(data)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown data conversion: {}\".format(to))\n",
    "    if converted is None:\n",
    "        raise TypeError('cannot handle data conversion of type: {} to {}'.format(type(data),to))\n",
    "    else:\n",
    "        return converted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function I did get from kaggle, they are needed to do the categorial correlation using cramers\n",
    "#https://stackoverflow.com/questions/46498455/categorical-features-correlation/46498792#46498792\n",
    "\n",
    "def correlation_ratio(categories, measurements):\n",
    "    fcat, _ = pd.factorize(categories)\n",
    "    cat_num = np.max(fcat)+1\n",
    "    y_avg_array = np.zeros(cat_num)\n",
    "    n_array = np.zeros(cat_num)\n",
    "    for i in range(0,cat_num):\n",
    "        cat_measures = measurements[np.argwhere(fcat == i).flatten()]\n",
    "        n_array[i] = len(cat_measures)\n",
    "        y_avg_array[i] = np.average(cat_measures)\n",
    "    y_total_avg = np.sum(np.multiply(y_avg_array,n_array))/np.sum(n_array)\n",
    "    numerator = np.sum(np.multiply(n_array,np.power(np.subtract(y_avg_array,y_total_avg),2)))\n",
    "    denominator = np.sum(np.power(np.subtract(measurements,y_total_avg),2))\n",
    "    if numerator == 0:\n",
    "        eta = 0.0\n",
    "    else:\n",
    "        eta = numerator/denominator\n",
    "    return eta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Associations for corr. matrix of categrocials and cat with num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function I did get from kaggle, they are needed to do the categorial correlation using cramers\n",
    "#https://stackoverflow.com/questions/46498455/categorical-features-correlation/46498792#46498792\n",
    "\n",
    "def associations(dataset, nominal_columns=None, mark_columns=False, theil_u=False, plot=True,\n",
    "                          return_results = False, **kwargs):\n",
    "    \"\"\"\n",
    "    Calculate the correlation/strength-of-association of features in data-set with both categorical (eda_tools) and\n",
    "    continuous features using:\n",
    "     - Pearson's R for continuous-continuous cases\n",
    "     - Correlation Ratio for categorical-continuous cases\n",
    "     - Cramer's V or Theil's U for categorical-categorical cases\n",
    "    :param dataset: NumPy ndarray / Pandas DataFrame\n",
    "        The data-set for which the features' correlation is computed\n",
    "    :param nominal_columns: string / list / NumPy ndarray\n",
    "        Names of columns of the data-set which hold categorical values. Can also be the string 'all' to state that all\n",
    "        columns are categorical, or None (default) to state none are categorical\n",
    "    :param mark_columns: Boolean (default: False)\n",
    "        if True, output's columns' names will have a suffix of '(nom)' or '(con)' based on there type (eda_tools or\n",
    "        continuous), as provided by nominal_columns\n",
    "    :param theil_u: Boolean (default: False)\n",
    "        In the case of categorical-categorical feaures, use Theil's U instead of Cramer's V\n",
    "    :param plot: Boolean (default: True)\n",
    "        If True, plot a heat-map of the correlation matrix\n",
    "    :param return_results: Boolean (default: False)\n",
    "        If True, the function will return a Pandas DataFrame of the computed associations\n",
    "    :param kwargs:\n",
    "        Arguments to be passed to used function and methods\n",
    "    :return: Pandas DataFrame\n",
    "        A DataFrame of the correlation/strength-of-association between all features\n",
    "    \"\"\"\n",
    "    dataset = convert(dataset, 'dataframe')\n",
    "    columns = dataset.columns\n",
    "    if nominal_columns is None:\n",
    "        nominal_columns = list()\n",
    "    elif nominal_columns == 'all':\n",
    "        nominal_columns = columns\n",
    "    corr = pd.DataFrame(index=columns, columns=columns)\n",
    "    for i in range(0,len(columns)):\n",
    "        for j in range(i,len(columns)):\n",
    "            if i == j:\n",
    "                corr[columns[i]][columns[j]] = 1.0\n",
    "            else:\n",
    "                if columns[i] in nominal_columns:\n",
    "                    if columns[j] in nominal_columns:\n",
    "                        if theil_u:\n",
    "                            corr[columns[j]][columns[i]] = theils_u(dataset[columns[i]],dataset[columns[j]])\n",
    "                            corr[columns[i]][columns[j]] = theils_u(dataset[columns[j]],dataset[columns[i]])\n",
    "                        else:\n",
    "                            cell = cramers_v(dataset[columns[i]],dataset[columns[j]])\n",
    "                            corr[columns[i]][columns[j]] = cell\n",
    "                            corr[columns[j]][columns[i]] = cell\n",
    "                    else:\n",
    "                        cell = correlation_ratio(dataset[columns[i]], dataset[columns[j]])\n",
    "                        corr[columns[i]][columns[j]] = cell\n",
    "                        corr[columns[j]][columns[i]] = cell\n",
    "                else:\n",
    "                    if columns[j] in nominal_columns:\n",
    "                        cell = correlation_ratio(dataset[columns[j]], dataset[columns[i]])\n",
    "                        corr[columns[i]][columns[j]] = cell\n",
    "                        corr[columns[j]][columns[i]] = cell\n",
    "                    else:\n",
    "                        cell, _ = ss.pearsonr(dataset[columns[i]], dataset[columns[j]])\n",
    "                        corr[columns[i]][columns[j]] = cell\n",
    "                        corr[columns[j]][columns[i]] = cell\n",
    "    corr.fillna(value=np.nan, inplace=True)\n",
    "    if mark_columns:\n",
    "        marked_columns = ['{} (nom)'.format(col) if col in nominal_columns else '{} (con)'.format(col) for col in columns]\n",
    "        corr.columns = marked_columns\n",
    "        corr.index = marked_columns\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10,10))#kwargs.get('figsize',None))\n",
    "        sns.heatmap(corr, annot=kwargs.get('annot',True), vmin=-1, vmax=1, fmt=kwargs.get('fmt','.2f'), cmap='seismic')\n",
    "        plt.show()\n",
    "    if return_results:\n",
    "        return corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### detecting outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from flo, not really tested\n",
    "def outliers(column, threshold = 3): #define a funciton like this, so there is a predefault value\n",
    "    '''labels oultiers, dedect outliers according to mean and standartddeviation \n",
    "    and a defined threshld which is the nuer of standarddeviations,taht are the outliers, outlier has to be removed still'''\n",
    "    return column[abs(column.apply(lambda x: (x-column.mean())/column.var() **(1/2))) >threshold] \n",
    "\n",
    "#CLV_outliers = outliers(nums['customer_lifetime_va'], 3)\n",
    "#MPA_outliers = outliers(nums['monthly_prem...'], 3)\n",
    "#to_drop = CLV_outliers.index | MPA_outliers.index\n",
    "#nums = nums.drop(to_drop)\n",
    "\n",
    "#own function, tested, learned that return column of dtatfreaem, then saz df['c'] = fungiyon()'\n",
    "def remove_outliers(df, feature, factor):\n",
    "    iqr = np.percentile(df[feature],75) - np.percentile(df[feature],25)\n",
    "    upper_limit = np.percentile(df[feature],75) + factor*iqr\n",
    "    lower_limit = np.percentile(df[feature],25) - factor*iqr\n",
    "    df[feature]=df[(df[feature]>lower_limit) & (df[feature]<upper_limit)]\n",
    "    return df[feature]\n",
    "\n",
    "#nums['customer_lifetime_va'] = remove_outliers(nums, 'customer_lifetime_va', 2.5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
